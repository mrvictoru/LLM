version: '0.3server'
services:
  llama_server:
    image: ghcr.io/ggerganov/llama.cpp:server-cuda
    command: [
      "-m", "/home/victoru/LLM/models/xLAM-7b-fc-r.Q4_K_S.gguf",
      "-c", "4096",
      "--host", "0.0.0.0",
      "--port", "8080",
      "--n-gpu-layers", "100"
    ]
    ports:
      - "8080:8080"
    volumes:
      - "./models:/models"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  python_dev_standalone:
    build:
      context: .
      dockerfile: LLM_server_setup/Dockerfile
    volumes:
      - "./src:/code"
    working_dir: /code
    command: ["jupyter", "notebook", "--port=8888", "--no-browser", "--ip=0.0.0.0", "--allow-root"]
    ports:
      - "8888:8888" # Optional: if you plan to run a Jupyter notebook or any web service
    environment:
      - OPENAI_API_BASE="http://llama_server:8080"
      - OPENAI_API_KEY=no_key
  
  mysql_db:
    image: mysql
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: classicmodels
      MYSQL_USER: user
      MYSQL_PASSWORD: password
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql
      - ./mysql-init:/docker-entrypoint-initdb.d

volumes:
  models:
  code:
  mysql_data:
