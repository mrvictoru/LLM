version: '0.4server-graph'
services:
  llama_server:
    image: ghcr.io/ggerganov/llama.cpp:server-cuda
    command: [
      "-m", "./models/Mistral-Nemo-Instruct-2407.Q6_K.gguf",
      "-t", "20",
      "-c", "4096",
      "--host", "0.0.0.0",
      "--port", "8080",
      "--n-gpu-layers", "200"
    ]
    ports:
      - "8080:8080"
    volumes:
      - "./models:/models"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  llama_server_2:
    image: ghcr.io/ggerganov/llama.cpp:server-cuda
    command: [
      "-m", "./models/Llama-3.2-3B-Instruct-Q6_K.gguf",
      "-t", "20",
      "-c", "4096",
      "--host", "0.0.0.0",
      "--port", "8081",
      "--n-gpu-layers", "200"
    ]
    ports:
      - "8081:8081"
    volumes:
      - "./models:/models"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  embedding_server:
    image: ghcr.io/ggerganov/llama.cpp:server-cuda
    command: [
      "-m", "./models/all-MiniLM-L6-v2-Q5_K_M.gguf",
      "-t", "20",
      "-c", "4096",
      "--host", "0.0.0.0",
      "--port", "8082",
      "--n-gpu-layers", "200",
      "--embedding"
    ]
    ports:
      - "8082:8082"
    volumes:
      - "./models:/models"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
  
  python_dev_graph:
    build:
      context: .
      dockerfile: LLM_graph_setup/Dockerfile
    volumes:
      - "./src:/code"
    working_dir: /code
    command: ["jupyter", "notebook", "--port=8888", "--no-browser", "--ip=0.0.0.0", "--allow-root", "--NotebookApp.token='llmpython_4698'"]
    ports:
      - "8888:8888" # Optional: if you plan to run a Jupyter notebook or any web service
    environment:
      - OPENAI_API_BASE="http://llama_server:8080"
      - OPENAI_API_KEY=no_key


volumes:
  models:
  code:
