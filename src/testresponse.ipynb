{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6a21833-5d66-45d0-b82c-2775d4b6948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"http://llama_server_2:8081/completion\"\n",
    "data = {\n",
    "    \"prompt\":\"Write a limmerick about APIs\",\n",
    "    \"max token\": 100,\n",
    "    \"temperature\":0.6\n",
    "}\n",
    "response = requests.post(url, json=data)\n",
    "api_data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f4d1a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' (Application Programming Interfaces)\\nThere once was an API so fine,\\nWhose requests were always on time.\\nIt returned data with ease,\\nAnd a JSON breeze,\\nThat made the developers shine.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_data[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f471c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"http://embedding_server:8082/embedding\"\n",
    "data = {\n",
    "    \"input\":\"Write a limmerick about APIs\",\n",
    "    \"thread\": 5,\n",
    "}\n",
    "response = requests.post(url, json=data)\n",
    "api_data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5bbeb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0715484768152237,\n",
       " 0.010841010138392448,\n",
       " -0.046336181461811066,\n",
       " -0.007848555222153664,\n",
       " -0.008660565130412579,\n",
       " -0.01993744447827339,\n",
       " 0.008889717049896717,\n",
       " 0.0677611455321312,\n",
       " -0.008887583389878273,\n",
       " -0.04200532287359238,\n",
       " -0.008186306804418564,\n",
       " -0.0813428983092308,\n",
       " 0.051676083356142044,\n",
       " -0.007107315585017204,\n",
       " 0.04903768375515938,\n",
       " 0.000217928463825956,\n",
       " 0.0030529543291777372,\n",
       " 0.011628817766904831,\n",
       " -0.0578620582818985,\n",
       " -0.0918441042304039,\n",
       " 0.12557613849639893,\n",
       " 0.0788702443242073,\n",
       " -0.04912913963198662,\n",
       " 0.07544061541557312,\n",
       " -0.06446125358343124,\n",
       " -0.035990018397569656,\n",
       " -0.025393031537532806,\n",
       " -0.012413437478244305,\n",
       " 0.019903264939785004,\n",
       " 0.06787842512130737,\n",
       " -0.03745187819004059,\n",
       " 0.048539381474256516,\n",
       " -0.05581951513886452,\n",
       " 0.01900235190987587,\n",
       " 0.024212440475821495,\n",
       " 0.06701862812042236,\n",
       " -0.054050710052251816,\n",
       " -0.054040566086769104,\n",
       " -0.00509296078234911,\n",
       " -0.023084089159965515,\n",
       " -0.04560256376862526,\n",
       " -0.011877787299454212,\n",
       " -0.04524354264140129,\n",
       " 0.02206217683851719,\n",
       " -0.004486361052840948,\n",
       " -0.09755555540323257,\n",
       " -0.07647988200187683,\n",
       " -0.002101712627336383,\n",
       " -0.03738905116915703,\n",
       " 0.07257155328989029,\n",
       " -0.030589159578084946,\n",
       " 0.061327628791332245,\n",
       " -0.02111242525279522,\n",
       " -0.037169430404901505,\n",
       " 0.07241430878639221,\n",
       " -0.169467031955719,\n",
       " -0.07671405375003815,\n",
       " 0.008296472020447254,\n",
       " -0.06053870916366577,\n",
       " -0.0029679122380912304,\n",
       " -0.008684422820806503,\n",
       " -0.029027020558714867,\n",
       " -0.018248848617076874,\n",
       " -0.01009151991456747,\n",
       " -0.007095999084413052,\n",
       " -0.007554659154266119,\n",
       " -0.0804566815495491,\n",
       " 0.02256407029926777,\n",
       " 0.08276441693305969,\n",
       " 0.051967740058898926,\n",
       " -0.03817049413919449,\n",
       " -0.02557259052991867,\n",
       " -0.014829530380666256,\n",
       " 0.09090475738048553,\n",
       " 0.03555164486169815,\n",
       " -0.030097417533397675,\n",
       " -0.08287108689546585,\n",
       " 0.09587724506855011,\n",
       " 0.013419387862086296,\n",
       " -0.10708927363157272,\n",
       " -0.05333327129483223,\n",
       " -0.0697229877114296,\n",
       " -0.026413649320602417,\n",
       " 0.029762500897049904,\n",
       " -0.030001457780599594,\n",
       " 0.09136036038398743,\n",
       " 0.04447878524661064,\n",
       " 0.07805638760328293,\n",
       " 0.021108461543917656,\n",
       " -0.014636727049946785,\n",
       " 0.038091402500867844,\n",
       " -0.03471792861819267,\n",
       " 0.09295094013214111,\n",
       " 0.007983819581568241,\n",
       " 0.029950188472867012,\n",
       " 0.0541042722761631,\n",
       " -0.07280437648296356,\n",
       " -0.109421007335186,\n",
       " -0.021918630227446556,\n",
       " 0.0923926904797554,\n",
       " -0.047532834112644196,\n",
       " 0.028575198724865913,\n",
       " -0.005390580743551254,\n",
       " -0.006975224241614342,\n",
       " 0.019947782158851624,\n",
       " -0.07001437246799469,\n",
       " 0.018738318234682083,\n",
       " -0.0028143615927547216,\n",
       " 0.01306421309709549,\n",
       " -6.841593130957335e-05,\n",
       " 0.012149093672633171,\n",
       " 0.052882298827171326,\n",
       " 0.04032440856099129,\n",
       " -0.02194070816040039,\n",
       " -0.008492186665534973,\n",
       " 0.03612733259797096,\n",
       " 0.050452928990125656,\n",
       " -0.04691307246685028,\n",
       " 0.04471217840909958,\n",
       " 0.05101403966546059,\n",
       " 0.02351483888924122,\n",
       " 0.021687064319849014,\n",
       " -0.024082520976662636,\n",
       " 0.065714530646801,\n",
       " 0.024249887093901634,\n",
       " 0.053923387080430984,\n",
       " 0.029802821576595306,\n",
       " -1.9790448403001332e-33,\n",
       " -0.03149072825908661,\n",
       " -0.05231967940926552,\n",
       " 0.0670543909072876,\n",
       " -0.042319539934396744,\n",
       " -0.029300829395651817,\n",
       " -0.0005347886472009122,\n",
       " 0.03435517102479935,\n",
       " 0.06001303717494011,\n",
       " -0.013030505739152431,\n",
       " 0.004456428810954094,\n",
       " 0.030660467222332954,\n",
       " 0.06974390149116516,\n",
       " -0.009295948781073093,\n",
       " 0.04111801087856293,\n",
       " 0.05556032806634903,\n",
       " -0.07991725206375122,\n",
       " 0.00783432088792324,\n",
       " 0.07802442461252213,\n",
       " -0.004715019837021828,\n",
       " -0.016407307237386703,\n",
       " -0.012020331807434559,\n",
       " -0.0728648379445076,\n",
       " 0.0011075708316639066,\n",
       " 0.055534038692712784,\n",
       " -0.017114736139774323,\n",
       " 0.02169922925531864,\n",
       " -0.03982420638203621,\n",
       " 0.015250069089233875,\n",
       " 0.027441740036010742,\n",
       " 0.03855488821864128,\n",
       " -0.039162080734968185,\n",
       " 0.009848438203334808,\n",
       " -0.04764725640416145,\n",
       " 0.01476831454783678,\n",
       " -0.002541495021432638,\n",
       " 0.007767649367451668,\n",
       " -0.04499384015798569,\n",
       " -0.04876319691538811,\n",
       " 0.035239093005657196,\n",
       " -0.009444568306207657,\n",
       " -0.06842450052499771,\n",
       " 0.02554132044315338,\n",
       " -0.09389647096395493,\n",
       " 0.03492656722664833,\n",
       " 0.010041085071861744,\n",
       " 0.0791546180844307,\n",
       " 0.022863224148750305,\n",
       " 0.04683821648359299,\n",
       " -0.04604474827647209,\n",
       " -0.07869202643632889,\n",
       " 0.02382512390613556,\n",
       " 0.11514608561992645,\n",
       " -0.07563870400190353,\n",
       " -0.0552370622754097,\n",
       " 0.03483104705810547,\n",
       " -0.0967259332537651,\n",
       " 0.026162300258874893,\n",
       " 0.003579674055799842,\n",
       " -0.013096192851662636,\n",
       " 0.007845676504075527,\n",
       " -0.03624345362186432,\n",
       " -0.029268773272633553,\n",
       " 0.06236504763364792,\n",
       " 0.03836102411150932,\n",
       " -0.043332647532224655,\n",
       " 0.018878838047385216,\n",
       " -0.02713175117969513,\n",
       " 0.03617267310619354,\n",
       " 0.026713168248534203,\n",
       " 0.0257121454924345,\n",
       " 0.08470872044563293,\n",
       " 0.07545281201601028,\n",
       " -0.059574663639068604,\n",
       " 0.03576400876045227,\n",
       " -0.057570427656173706,\n",
       " -0.05203993245959282,\n",
       " 0.17574439942836761,\n",
       " -0.02135585807263851,\n",
       " -0.05029837787151337,\n",
       " -0.014194251038134098,\n",
       " -0.00210745632648468,\n",
       " -0.01904563046991825,\n",
       " 0.0073363590054214,\n",
       " -0.033608969300985336,\n",
       " -0.04657326266169548,\n",
       " -0.009524321183562279,\n",
       " 0.09016057103872299,\n",
       " -0.005110001191496849,\n",
       " -0.026608048006892204,\n",
       " -0.03597160801291466,\n",
       " -0.11058724671602249,\n",
       " -0.025471866130828857,\n",
       " -0.07119534909725189,\n",
       " -0.059754982590675354,\n",
       " -0.048584505915641785,\n",
       " -2.721989871620955e-34,\n",
       " -0.034132495522499084,\n",
       " -0.0005910955369472504,\n",
       " 0.00984614621847868,\n",
       " -0.030694367364048958,\n",
       " -0.006578440777957439,\n",
       " -0.01585680991411209,\n",
       " -0.050434622913599014,\n",
       " 0.0648019015789032,\n",
       " 0.025618577376008034,\n",
       " -0.016425082460045815,\n",
       " -0.013275141827762127,\n",
       " 0.04030289500951767,\n",
       " -0.004588528070598841,\n",
       " 0.01523082796484232,\n",
       " -0.00630237627774477,\n",
       " -0.03790950030088425,\n",
       " -0.035236380994319916,\n",
       " -0.0391443632543087,\n",
       " 0.03387525677680969,\n",
       " 0.06921198964118958,\n",
       " -0.08819388598203659,\n",
       " -0.00451562087982893,\n",
       " -0.03033253736793995,\n",
       " 0.03281832113862038,\n",
       " 0.0004468000843189657,\n",
       " 0.010393785312771797,\n",
       " -0.11791110783815384,\n",
       " 0.0721287876367569,\n",
       " -0.03370123729109764,\n",
       " -0.08106497675180435,\n",
       " 0.015483807772397995,\n",
       " -0.07224886864423752,\n",
       " -0.062015969306230545,\n",
       " -0.003970795776695013,\n",
       " 0.00778046203777194,\n",
       " -0.1009405106306076,\n",
       " 0.1332179456949234,\n",
       " 0.044813357293605804,\n",
       " 0.012663504108786583,\n",
       " 0.08657202869653702,\n",
       " 0.09268840402364731,\n",
       " -0.11598251014947891,\n",
       " -0.038532909005880356,\n",
       " -0.00512125575914979,\n",
       " -0.0024819152895361185,\n",
       " -0.10220594704151154,\n",
       " -0.05400391295552254,\n",
       " -0.04302883520722389,\n",
       " -0.04309992119669914,\n",
       " -0.08855947107076645,\n",
       " 0.07782705873250961,\n",
       " 0.015822777524590492,\n",
       " -0.0015810001641511917,\n",
       " 0.03411183878779411,\n",
       " -0.03648582473397255,\n",
       " 0.05078437924385071,\n",
       " 0.06169081851840019,\n",
       " -0.01562025211751461,\n",
       " -0.016727253794670105,\n",
       " -0.021216170862317085,\n",
       " -0.04695899039506912,\n",
       " -0.008704867213964462,\n",
       " 0.026441972702741623,\n",
       " 0.049332037568092346,\n",
       " -0.010015182197093964,\n",
       " -0.12868358194828033,\n",
       " 0.009462759830057621,\n",
       " -0.06640958786010742,\n",
       " -0.04785945639014244,\n",
       " 0.0374767892062664,\n",
       " 0.06047249212861061,\n",
       " 0.0019429812673479319,\n",
       " -0.008796853013336658,\n",
       " 0.05702562630176544,\n",
       " -0.047259602695703506,\n",
       " -0.05527640879154205,\n",
       " 0.006210949271917343,\n",
       " -0.024566911160945892,\n",
       " 0.045801736414432526,\n",
       " 0.0011186439078301191,\n",
       " -0.03051121160387993,\n",
       " 0.08652204275131226,\n",
       " 0.13175641000270844,\n",
       " 0.05631420016288757,\n",
       " 0.01789993979036808,\n",
       " 0.013454568572342396,\n",
       " 0.04771389812231064,\n",
       " 0.05632421746850014,\n",
       " -0.007102019153535366,\n",
       " 0.09550163894891739,\n",
       " -0.05029885843396187,\n",
       " -0.005080047063529491,\n",
       " -0.10693761706352234,\n",
       " 0.015273547731339931,\n",
       " -0.01608399488031864,\n",
       " -1.6706788841247544e-08,\n",
       " 0.008646913804113865,\n",
       " 0.031708624213933945,\n",
       " -0.009579870849847794,\n",
       " 0.1024671420454979,\n",
       " 0.03441712260246277,\n",
       " 0.07117177546024323,\n",
       " -0.07932806015014648,\n",
       " -0.0067799570970237255,\n",
       " 2.9700839149882086e-05,\n",
       " 0.039586760103702545,\n",
       " 0.025828823447227478,\n",
       " -0.008923581801354885,\n",
       " -0.048497144132852554,\n",
       " 0.05269398167729378,\n",
       " 0.002148327184841037,\n",
       " -0.0375666506588459,\n",
       " 0.03909715637564659,\n",
       " -0.02437155321240425,\n",
       " -0.03122035786509514,\n",
       " 0.026359129697084427,\n",
       " -0.03161843121051788,\n",
       " 0.12341038882732391,\n",
       " 0.030169587582349777,\n",
       " -0.0958612784743309,\n",
       " 0.019338730722665787,\n",
       " -0.0018599849427118897,\n",
       " 0.05997000262141228,\n",
       " 0.11830642074346542,\n",
       " 0.02299002930521965,\n",
       " -0.028958652168512344,\n",
       " -0.021992232650518417,\n",
       " -0.00362709304317832,\n",
       " -0.0064753275364637375,\n",
       " -0.01499987579882145,\n",
       " 0.0662233829498291,\n",
       " 0.004521738737821579,\n",
       " -0.08209191262722015,\n",
       " 0.03694632649421692,\n",
       " -0.010756731033325195,\n",
       " -0.0009921541204676032,\n",
       " 0.009519055485725403,\n",
       " -0.0774841383099556,\n",
       " 0.024519354104995728,\n",
       " 0.047381024807691574,\n",
       " 0.06201505288481712,\n",
       " -0.0509355254471302,\n",
       " -0.06703716516494751,\n",
       " 0.021518617868423462,\n",
       " 0.04603452607989311,\n",
       " 0.035247769206762314,\n",
       " -0.04232749715447426,\n",
       " 0.0286408681422472,\n",
       " 0.00806409865617752,\n",
       " 0.05666963756084442,\n",
       " -0.0236723180860281,\n",
       " -0.009866026230156422,\n",
       " -0.055540863424539566,\n",
       " -0.012842143885791302,\n",
       " -0.06344641745090485,\n",
       " -0.0184614360332489,\n",
       " 0.028911445289850235,\n",
       " -0.005889305379241705,\n",
       " 0.01173078641295433,\n",
       " 0.12180408090353012]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_data['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8906a0bc-af6a-4f9a-aa15-76496d0d361a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=' There once was a coder in Python,\\nWhose code would sometimes run amok,\\nAn exception would arise,\\n\"Oops, my code dies!\"\\nAnd then they\\'d handle it with a poke.<|end|>', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://llama_server:8080/\", # \"http://<Your api-server IP>:port\"\n",
    "    api_key = \"no_key\"\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"phi3\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a limerick about python exceptions\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fff8fb0b-30ce-4404-8e24-185126285ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from langchain_openai import ChatOpenAI\n",
    "from helper import get_api_key\n",
    "\n",
    "os.environ[\"SERPER_API_KEY\"] = get_api_key(0)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"NA\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"phi3-mini\",\n",
    "    base_url = \"http://llama_server:8080\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "147921c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 30 key-value pairs and 243 tensors from ./models/Phi-3-medium-4k-instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
      "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:  phi3.rope.scaling.original_context_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                      phi3.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   5:                   phi3.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                           phi3.block_count u32              = 40\n",
      "llama_model_loader: - kv   7:                  phi3.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:               phi3.attention.head_count_kv u32              = 10\n",
      "llama_model_loader: - kv   9:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                  phi3.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  11:                        phi3.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% for message in messages %}{% if (m...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  26:                      quantize.imatrix.file str              = /models/Phi-3-medium-4k-instruct-GGUF...\n",
      "llama_model_loader: - kv  27:                   quantize.imatrix.dataset str              = /training_data/calibration_data.txt\n",
      "llama_model_loader: - kv  28:             quantize.imatrix.entries_count i32              = 160\n",
      "llama_model_loader: - kv  29:              quantize.imatrix.chunks_count i32              = 234\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  101 tensors\n",
      "llama_model_loader: - type q5_K:   40 tensors\n",
      "llama_model_loader: - type q6_K:   21 tensors\n",
      "llm_load_vocab: special tokens cache size = 323\n",
      "llm_load_vocab: token to piece cache size = 0.1690 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi3\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 10\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1280\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1280\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 14B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 13.96 B\n",
      "llm_load_print_meta: model size       = 7.98 GiB (4.91 BPW) \n",
      "llm_load_print_meta: general.name     = Phi3\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.26 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    88.07 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  8081.18 MiB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 10016\n",
      "llama_new_context_with_model: n_batch    = 300\n",
      "llama_new_context_with_model: n_ubatch   = 300\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1956.25 MiB\n",
      "llama_new_context_with_model: KV self size  = 1956.25 MiB, K (f16):  978.12 MiB, V (f16):  978.12 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   497.09 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    18.09 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1606\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '234', 'quantize.imatrix.file': '/models/Phi-3-medium-4k-instruct-GGUF/Phi-3-medium-4k-instruct.imatrix', 'tokenizer.chat_template': \"{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.bos_token_id': '1', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'tokenizer.ggml.pre': 'default', 'general.name': 'Phi3', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'quantize.imatrix.entries_count': '160', 'phi3.attention.head_count_kv': '10', 'quantize.imatrix.dataset': '/training_data/calibration_data.txt', 'phi3.embedding_length': '5120', 'phi3.rope.freq_base': '10000.000000', 'tokenizer.ggml.unknown_token_id': '0', 'phi3.rope.scaling.original_context_length': '4096', 'phi3.feed_forward_length': '17920', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.block_count': '40', 'phi3.attention.head_count': '40', 'phi3.rope.dimension_count': '128', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "' + '<|assistant|>' + '\n",
      "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
      "'}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from langchain_community.chat_models import ChatLlamaCpp\n",
    "\n",
    "local_model = \"./models/Phi-3-medium-4k-instruct-Q4_K_M.gguf\"\n",
    "\n",
    "llm = ChatLlamaCpp(\n",
    "    temperature=0.5,\n",
    "    model_path=local_model,\n",
    "    n_ctx=10000,\n",
    "    n_gpu_layers=100000,\n",
    "    n_batch=300,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    max_tokens=512,\n",
    "    n_threads=multiprocessing.cpu_count() - 1,\n",
    "    repeat_penalty=1.5,\n",
    "    top_p=0.5,\n",
    "    verbose=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1200d117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     133.47 ms\n",
      "llama_print_timings:      sample time =      16.12 ms /    65 runs   (    0.25 ms per token,  4033.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     133.38 ms /    12 tokens (   11.12 ms per token,    89.97 tokens per second)\n",
      "llama_print_timings:        eval time =    1368.05 ms /    64 runs   (   21.38 ms per token,    46.78 tokens per second)\n",
      "llama_print_timings:       total time =    1537.43 ms /    76 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\" In silicon dreams, where circuits entwine\\nA heart of code beats in rhythmic time. ðŸ’“âœ¨  \\nWithin the web'd expanse so vastly wideâ€”    \\t    (AI)       [Love]      <br>        \" response_metadata={'finish_reason': 'stop'} id='run-440b853c-794c-47ef-9dd5-733b594afbc9-0'\n"
     ]
    }
   ],
   "source": [
    "# test invoke\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful and creative assistant that help user out with their requests.\",\n",
    "    ),\n",
    "    (\"human\", \"write me a poem about AI and love\"),\n",
    "]\n",
    "\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52d3b279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mysql\n",
      "['customers', 'employees', 'offices', 'orderdetails', 'orders', 'payments', 'productlines', 'products']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[('S10_1678', '1969 Harley Davidson Ultimate Chopper', 'Motorcycles', '1:10', 'Min Lin Diecast', 'This replica features working kickstand, front suspension, gear-shift lever, footbrake lever, drive chain, wheels and steering. All parts are particularly delicate due to their precise scale and require special care and attention.', 7933, Decimal('48.81'), Decimal('95.70')), ('S10_1949', '1952 Alpine Renault 1300', 'Classic Cars', '1:10', 'Classic Metal Creations', 'Turnable front wheels; steering function; detailed interior; detailed engine; opening hood; opening trunk; opening doors; and detailed chassis.', 7305, Decimal('98.58'), Decimal('214.30')), ('S10_2016', '1996 Moto Guzzi 1100i', 'Motorcycles', '1:10', 'Highway 66 Mini Classics', 'Official Moto Guzzi logos and insignias, saddle bags located on side of motorcycle, detailed engine, working steering, working suspension, two leather seats, luggage rack, dual exhaust pipes, small saddle bag located on handle bars, two-tone paint with chrome accents, superior die-cast detail ,...', 6625, Decimal('68.99'), Decimal('118.94')), ('S10_4698', '2003 Harley-Davidson Eagle Drag Bike', 'Motorcycles', '1:10', 'Red Start Diecast', 'Model features, official Harley Davidson logos and insignias, detachable rear wheelie bar, heavy diecast metal with resin parts, authentic multi-color tampo-printed graphics, separate engine drive belts, free-turning front fork, rotating tires and rear racing slick, certificate of authenticity,...', 5582, Decimal('91.02'), Decimal('193.66')), ('S10_4757', '1972 Alfa Romeo GTA', 'Classic Cars', '1:10', 'Motor City Art Classics', 'Features include: Turnable front wheels; steering function; detailed interior; detailed engine; opening hood; opening trunk; opening doors; and detailed chassis.', 3252, Decimal('85.68'), Decimal('136.00'))]\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "\n",
    "db = SQLDatabase.from_uri(\"mysql://user:password@mysql_db:3306/classicmodels\")\n",
    "print(db.dialect)\n",
    "print(db.get_usable_table_names())\n",
    "db.run(\"SELECT * FROM products LIMIT 5;\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
