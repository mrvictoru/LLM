{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"A programmer, quite keen, did code,\\nWith Python, his skills on display.\\nBut errors arose,\\nLike a tempest that blows,\\nExceptions that made his code stray. \\n\\n\\nLet me know if you'd like another limerick or have any other requests! \\n\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://llama_server:8080/\", # \"http://<Your api-server IP>:port\"\n",
    "    api_key = \"no_key\"\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"xLAM7b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a limerick about python exceptions\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a base class for an LLM agent\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class LLMAgent:\n",
    "    def __init__(self, client: openai.OpenAI, model: str = \"xLAM7b\", system_message: str = \"\") -> None:\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.sys_msg = system_message\n",
    "        self.messages: list[dict] = []\n",
    "        if system_message:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    \n",
    "    def __call__(self, msg: str) -> str:\n",
    "        if msg:\n",
    "            self.messages.append({\"role\": \"user\", \"content\": msg})\n",
    "        output = self.inference()\n",
    "        self.messages.append({\"role\": \"system\", \"content\": output})\n",
    "        return output\n",
    "    \n",
    "    def inference(self) -> str:\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages\n",
    "        )\n",
    "        return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You run in a loop of Thought, Action, PAUSE, Observation.\n",
    "At the end of the loop you output an Answer\n",
    "Use Thought to describe your thoughts about the question you have been asked.\n",
    "Use Action to run one of the actions available to you - then return PAUSE.\n",
    "Observation will be the result of running those actions.\n",
    "\n",
    "Your available tools are:\n",
    "\n",
    "{actions}\n",
    "\n",
    "Example session:\n",
    "\n",
    "Question: {question}\n",
    "Thought: {thought}\n",
    "Action: {action}\n",
    "PAUSE \n",
    "\n",
    "You will be called again with this:\n",
    "\n",
    "Observation: {observation}\n",
    "\n",
    "Thought: {thought}\n",
    "Action: {action}\n",
    "PAUSE\n",
    "\n",
    "You will be called again with this: \n",
    "\n",
    "Observation: {observation}\n",
    "\n",
    "If you have the answer, output it as the Answer.\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Now it's your turn:\n",
    "\"\"\".strip()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
