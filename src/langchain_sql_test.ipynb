{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell if langsmith is used to trace the chain\n",
    "import os\n",
    "from helper import get_api_key\n",
    "\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = get_api_key(1)\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'default'\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_community.llms.llamacpp import LlamaCpp\n",
    "from langchain_community.embeddings.llamacpp import LlamaCppEmbeddings\n",
    "\n",
    "class LlamaCppEmbeddings_(LlamaCppEmbeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed a list of documents using the Llama model.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        embeddings = [self.client.embed(text)[0] for text in texts]\n",
    "        return [list(map(float, e)) for e in embeddings]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed a query using the Llama model.\n",
    "\n",
    "        Args:\n",
    "            text: The text to embed.\n",
    "\n",
    "        Returns:\n",
    "            Embeddings for the text.\n",
    "        \"\"\"\n",
    "        embedding = self.client.embed(text)[0]\n",
    "        return list(map(float, embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 32 key-value pairs and 363 tensors from ./models/Mistral-Nemo-Instruct-2407.Q5_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Models\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 12B\n",
      "llama_model_loader: - kv   4:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   5:                          general.languages arr[str,9]       = [\"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", ...\n",
      "llama_model_loader: - kv   6:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   7:                       llama.context_length u32              = 1024000\n",
      "llama_model_loader: - kv   8:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  11:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  14:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  15:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 16\n",
      "llama_model_loader: - kv  17:                           llama.vocab_size u32              = 131072\n",
      "llama_model_loader: - kv  18:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = tekken\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,131072]  = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,131072]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "Exception ignored on calling ctypes callback function: <function llama_log_callback at 0x745bad7ca560>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/llama_cpp/_logger.py\", line 30, in llama_log_callback\n",
      "    print(text.decode(\"utf-8\"), end=\"\", flush=True, file=sys.stderr)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc4 in position 128: invalid continuation byte\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {%- if messages[0]['role'] == 'system...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q5_K:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 1000\n",
      "llm_load_vocab: token to piece cache size = 0.8498 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 131072\n",
      "llm_load_print_meta: n_merges         = 269443\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 1024000\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 1024000\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Small\n",
      "llm_load_print_meta: model params     = 12.25 B\n",
      "llm_load_print_meta: model size       = 7.93 GiB (5.56 BPW) \n",
      "llm_load_print_meta: general.name     = Models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 1196 'Ä'\n",
      "llm_load_print_meta: max token length = 150\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.34 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   440.00 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  7676.58 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 5024\n",
      "llama_new_context_with_model: n_batch    = 1000\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   785.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  785.00 MiB, K (f16):  392.50 MiB, V (f16):  392.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.50 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   359.82 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    19.82 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n\\n{{- bos_token }}\\n{%- for message in loop_messages %}\\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\\n        {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\\n    {%- endif %}\\n    {%- if message['role'] == 'user' %}\\n        {%- if loop.last and system_message is defined %}\\n            {{- '[INST] ' + system_message + '\\\\n\\\\n' + message['content'] + '[/INST]' }}\\n        {%- else %}\\n            {{- '[INST] ' + message['content'] + '[/INST]' }}\\n        {%- endif %}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- ' ' + message['content'] + eos_token}}\\n    {%- else %}\\n        {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\\n    {%- endif %}\\n{%- endfor %}\\n\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '131072', 'general.file_type': '16', 'llama.attention.key_length': '128', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'tekken', 'llama.context_length': '1024000', 'general.name': 'Models', 'general.type': 'model', 'general.size_label': '12B', 'llama.attention.value_length': '128', 'general.license': 'apache-2.0', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '5120', 'llama.block_count': '40', 'llama.attention.head_count_kv': '8'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content'] %}\n",
      "    {%- set loop_messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set loop_messages = messages %}\n",
      "{%- endif %}\n",
      "\n",
      "{{- bos_token }}\n",
      "{%- for message in loop_messages %}\n",
      "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n",
      "        {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n",
      "    {%- endif %}\n",
      "    {%- if message['role'] == 'user' %}\n",
      "        {%- if loop.last and system_message is defined %}\n",
      "            {{- '[INST] ' + system_message + '\\n\\n' + message['content'] + '[/INST]' }}\n",
      "        {%- else %}\n",
      "            {{- '[INST] ' + message['content'] + '[/INST]' }}\n",
      "        {%- endif %}\n",
      "    {%- elif message['role'] == 'assistant' %}\n",
      "        {{- ' ' + message['content'] + eos_token}}\n",
      "    {%- else %}\n",
      "        {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n",
      "llama_model_loader: loaded meta data with 32 key-value pairs and 363 tensors from ./models/Mistral-Nemo-Instruct-2407.Q5_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Models\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 12B\n",
      "llama_model_loader: - kv   4:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   5:                          general.languages arr[str,9]       = [\"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", ...\n",
      "llama_model_loader: - kv   6:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   7:                       llama.context_length u32              = 1024000\n",
      "llama_model_loader: - kv   8:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  11:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  14:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  15:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 16\n",
      "llama_model_loader: - kv  17:                           llama.vocab_size u32              = 131072\n",
      "llama_model_loader: - kv  18:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = tekken\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,131072]  = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,131072]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "Exception ignored on calling ctypes callback function: <function llama_log_callback at 0x745bad7ca560>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/llama_cpp/_logger.py\", line 30, in llama_log_callback\n",
      "    print(text.decode(\"utf-8\"), end=\"\", flush=True, file=sys.stderr)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc4 in position 128: invalid continuation byte\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {%- if messages[0]['role'] == 'system...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q5_K:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 1000\n",
      "llm_load_vocab: token to piece cache size = 0.8498 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 131072\n",
      "llm_load_print_meta: n_merges         = 269443\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 1024000\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 1024000\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Small\n",
      "llm_load_print_meta: model params     = 12.25 B\n",
      "llm_load_print_meta: model size       = 7.93 GiB (5.56 BPW) \n",
      "llm_load_print_meta: general.name     = Models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 1196 'Ä'\n",
      "llm_load_print_meta: max token length = 150\n",
      "llm_load_tensors: ggml ctx size =    0.17 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  8116.58 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =    80.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   80.00 MiB, K (f16):   40.00 MiB, V (f16):   40.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.02 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   791.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    11.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 444\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content'] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n\\n{{- bos_token }}\\n{%- for message in loop_messages %}\\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\\n        {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\\n    {%- endif %}\\n    {%- if message['role'] == 'user' %}\\n        {%- if loop.last and system_message is defined %}\\n            {{- '[INST] ' + system_message + '\\\\n\\\\n' + message['content'] + '[/INST]' }}\\n        {%- else %}\\n            {{- '[INST] ' + message['content'] + '[/INST]' }}\\n        {%- endif %}\\n    {%- elif message['role'] == 'assistant' %}\\n        {{- ' ' + message['content'] + eos_token}}\\n    {%- else %}\\n        {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\\n    {%- endif %}\\n{%- endfor %}\\n\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '131072', 'general.file_type': '16', 'llama.attention.key_length': '128', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'tekken', 'llama.context_length': '1024000', 'general.name': 'Models', 'general.type': 'model', 'general.size_label': '12B', 'llama.attention.value_length': '128', 'general.license': 'apache-2.0', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '5120', 'llama.block_count': '40', 'llama.attention.head_count_kv': '8'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content'] %}\n",
      "    {%- set loop_messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set loop_messages = messages %}\n",
      "{%- endif %}\n",
      "\n",
      "{{- bos_token }}\n",
      "{%- for message in loop_messages %}\n",
      "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n",
      "        {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n",
      "    {%- endif %}\n",
      "    {%- if message['role'] == 'user' %}\n",
      "        {%- if loop.last and system_message is defined %}\n",
      "            {{- '[INST] ' + system_message + '\\n\\n' + message['content'] + '[/INST]' }}\n",
      "        {%- else %}\n",
      "            {{- '[INST] ' + message['content'] + '[/INST]' }}\n",
      "        {%- endif %}\n",
      "    {%- elif message['role'] == 'assistant' %}\n",
      "        {{- ' ' + message['content'] + eos_token}}\n",
      "    {%- else %}\n",
      "        {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "# using local installed llama_cpp_python\n",
    "import multiprocessing\n",
    "\n",
    "from langchain_community.chat_models import ChatLlamaCpp\n",
    "\n",
    "local_model = \"./models/Mistral-Nemo-Instruct-2407.Q5_K_S.gguf\"\n",
    "\n",
    "llm = ChatLlamaCpp(\n",
    "    temperature=0.3,\n",
    "    model_path=local_model,\n",
    "    n_ctx=5000,\n",
    "    n_gpu_layers=100,\n",
    "    n_batch=1000,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    max_tokens=768,\n",
    "    n_threads=multiprocessing.cpu_count() - 1,\n",
    "    repeat_penalty=1,\n",
    "    top_p=0.8\n",
    ")\n",
    "\n",
    "#Use local model for embedding\n",
    "embeddings = LlamaCppEmbeddings_(model_path=local_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using locally served llama_cpp with openai wrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_base=\"http://llama_server:8080/\",\n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "import openai\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://llama_server:8080/\", # \"http://<Your api-server IP>:port\"\n",
    "    api_key = \"no_key\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mysql\n",
      "['customers', 'employees', 'offices', 'orderdetails', 'orders', 'payments', 'productlines', 'products']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[('S10_1678', '1969 Harley Davidson Ultimate Chopper', 'Motorcycles', '1:10', 'Min Lin Diecast', 'This replica features working kickstand, front suspension, gear-shift lever, footbrake lever, drive chain, wheels and steering. All parts are particularly delicate due to their precise scale and require special care and attention.', 7933, Decimal('48.81'), Decimal('95.70')), ('S10_1949', '1952 Alpine Renault 1300', 'Classic Cars', '1:10', 'Classic Metal Creations', 'Turnable front wheels; steering function; detailed interior; detailed engine; opening hood; opening trunk; opening doors; and detailed chassis.', 7305, Decimal('98.58'), Decimal('214.30')), ('S10_2016', '1996 Moto Guzzi 1100i', 'Motorcycles', '1:10', 'Highway 66 Mini Classics', 'Official Moto Guzzi logos and insignias, saddle bags located on side of motorcycle, detailed engine, working steering, working suspension, two leather seats, luggage rack, dual exhaust pipes, small saddle bag located on handle bars, two-tone paint with chrome accents, superior die-cast detail ,...', 6625, Decimal('68.99'), Decimal('118.94')), ('S10_4698', '2003 Harley-Davidson Eagle Drag Bike', 'Motorcycles', '1:10', 'Red Start Diecast', 'Model features, official Harley Davidson logos and insignias, detachable rear wheelie bar, heavy diecast metal with resin parts, authentic multi-color tampo-printed graphics, separate engine drive belts, free-turning front fork, rotating tires and rear racing slick, certificate of authenticity,...', 5582, Decimal('91.02'), Decimal('193.66')), ('S10_4757', '1972 Alfa Romeo GTA', 'Classic Cars', '1:10', 'Motor City Art Classics', 'Features include: Turnable front wheels; steering function; detailed interior; detailed engine; opening hood; opening trunk; opening doors; and detailed chassis.', 3252, Decimal('85.68'), Decimal('136.00'))]\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the SQL utilities from langchain\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "\n",
    "db = SQLDatabase.from_uri(\"mysql://user:password@mysql_db:3306/classicmodels\")\n",
    "print(db.dialect)\n",
    "print(db.get_usable_table_names())\n",
    "db.run(\"SELECT * FROM products LIMIT 5;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.run(\"SELECT buyPrice from products where productName = '1969 Harley Davidson Ultimate Chopper';\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.run(\"SELECT firstName, lastName, jobTitle FROM employees WHERE jobTitle LIKE '%Marketing%';\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     861.23 ms\n",
      "llama_print_timings:      sample time =       0.90 ms /    14 runs   (    0.06 ms per token, 15555.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2295.43 ms /  2939 tokens (    0.78 ms per token,  1280.37 tokens per second)\n",
      "llama_print_timings:        eval time =     315.47 ms /    13 runs   (   24.27 ms per token,    41.21 tokens per second)\n",
      "llama_print_timings:       total time =    2632.12 ms /  2952 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT price\n",
      "FROM products\n",
      "WHERE productCode =01 =\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_sql_query_chain\n",
    "\n",
    "generate_query = create_sql_query_chain(llm,db)\n",
    "query = generate_query.invoke({\"question\": \"What is the price of '1969 Harley Davidson Ultimate Chopper'?\"})\n",
    "print(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "execute_query = QuerySQLDataBaseTool(db=db)\n",
    "execute_query.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     533.01 ms\n",
      "llama_print_timings:      sample time =       2.18 ms /    73 runs   (    0.03 ms per token, 33501.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =      49.95 ms /    30 tokens (    1.66 ms per token,   600.64 tokens per second)\n",
      "llama_print_timings:        eval time =    1351.18 ms /    72 runs   (   18.77 ms per token,    53.29 tokens per second)\n",
      "llama_print_timings:       total time =    1455.42 ms /   102 tokens\n",
      "Error closing cursor\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/base.py\", line 2213, in _safe_close_cursor\n",
      "    cursor.close()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/MySQLdb/cursors.py\", line 83, in close\n",
      "    while self.nextset():\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/MySQLdb/cursors.py\", line 137, in nextset\n",
      "    nr = db.next_result()\n",
      "MySQLdb.ProgrammingError: (1064, \"You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'SQLResult: 48.81\\nAnswer: The price of the '1969 Harley Davidson Ultimate Chopper' at line 1\")\n"
     ]
    }
   ],
   "source": [
    "test_chain = generate_query | execute_query\n",
    "result = test_chain.invoke({\"question\": \"What is the price of the '1969 Harley Davidson Ultimate Chopper'?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_chain.get_prompts()[0].pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     533.01 ms\n",
      "llama_print_timings:      sample time =       2.11 ms /    73 runs   (    0.03 ms per token, 34548.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1779.20 ms /  3266 tokens (    0.54 ms per token,  1835.66 tokens per second)\n",
      "llama_print_timings:        eval time =    1343.44 ms /    72 runs   (   18.66 ms per token,    53.59 tokens per second)\n",
      "llama_print_timings:       total time =    3169.33 ms /  3338 tokens\n",
      "Error closing cursor\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/base.py\", line 2213, in _safe_close_cursor\n",
      "    cursor.close()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/MySQLdb/cursors.py\", line 83, in close\n",
      "    while self.nextset():\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/MySQLdb/cursors.py\", line 137, in nextset\n",
      "    nr = db.next_result()\n",
      "MySQLdb.ProgrammingError: (1064, \"You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'SQLResult: 48.81\\nAnswer: The price of the '1969 Harley Davidson Ultimate Chopper' at line 1\")\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     533.01 ms\n",
      "llama_print_timings:      sample time =       0.82 ms /    28 runs   (    0.03 ms per token, 34063.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =      73.69 ms /   149 tokens (    0.49 ms per token,  2022.12 tokens per second)\n",
      "llama_print_timings:        eval time =     401.71 ms /    27 runs   (   14.88 ms per token,    67.21 tokens per second)\n",
      "llama_print_timings:       total time =     489.00 ms /   176 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The price of the '1969 Harley Davidson Ultimate Chopper' is 48.81.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Result: {result}\n",
    "Answer: \"\"\"\n",
    ")\n",
    "\n",
    "# create first chain, where the template prompt get passed to the llm, the output is extract as str with output parser\n",
    "rephrase_answer = answer_prompt | llm | StrOutputParser()\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(query=generate_query).assign(\n",
    "        result=itemgetter(\"query\") | execute_query\n",
    "    )\n",
    "    | rephrase_answer\n",
    ")\n",
    "\n",
    "#query = generate_query.invoke({\"question\": \"What is the price of the '1969 Harley Davidson Ultimate Chopper'?\"})\n",
    "#print(query)\n",
    "#result = execute_query.invoke(query)\n",
    "#print(result)\n",
    "\n",
    "\n",
    "chain.invoke({\"question\": \"What is the price of the '1969 Harley Davidson Ultimate Chopper'?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.get_prompts()[0].pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.get_prompts()[1].pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of example prompts from sql_examples.json\n",
    "# read json file\n",
    "import json\n",
    "with open(\"sql_examples.json\", \"r\") as read_file:\n",
    "    examples = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     661.29 ms /     8 tokens (   82.66 ms per token,    12.10 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     668.82 ms /     9 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     733.25 ms /    12 tokens (   61.10 ms per token,    16.37 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     739.33 ms /    13 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     929.01 ms /    12 tokens (   77.42 ms per token,    12.92 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     934.88 ms /    13 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     869.63 ms /    11 tokens (   79.06 ms per token,    12.65 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     879.69 ms /    12 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     577.86 ms /    10 tokens (   57.79 ms per token,    17.31 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     585.50 ms /    11 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     623.64 ms /    10 tokens (   62.36 ms per token,    16.03 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     631.41 ms /    11 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1593.07 ms /    22 tokens (   72.41 ms per token,    13.81 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1602.65 ms /    23 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     530.12 ms /     9 tokens (   58.90 ms per token,    16.98 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     535.08 ms /    10 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1082.66 ms /    17 tokens (   63.69 ms per token,    15.70 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1091.73 ms /    18 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1079.40 ms /    19 tokens (   56.81 ms per token,    17.60 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1087.50 ms /    20 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1154.36 ms /    19 tokens (   60.76 ms per token,    16.46 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1164.31 ms /    20 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     544.30 ms /     9 tokens (   60.48 ms per token,    16.54 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     551.30 ms /    10 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1113.53 ms /    16 tokens (   69.60 ms per token,    14.37 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1122.11 ms /    17 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1181.66 ms /    15 tokens (   78.78 ms per token,    12.69 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1188.98 ms /    16 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     830.66 ms /    10 tokens (   83.07 ms per token,    12.04 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     837.28 ms /    11 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     925.67 ms /    16 tokens (   57.85 ms per token,    17.28 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     934.72 ms /    17 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1375.24 ms /    23 tokens (   59.79 ms per token,    16.72 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1384.20 ms /    24 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     553.05 ms /     9 tokens (   61.45 ms per token,    16.27 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     561.06 ms /    10 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     923.00 ms /    17 tokens (   54.29 ms per token,    18.42 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     930.97 ms /    18 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1141.98 ms /    19 tokens (   60.10 ms per token,    16.64 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1149.23 ms /    20 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =    1106.65 ms /    19 tokens (   58.24 ms per token,    17.17 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1114.60 ms /    20 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     596.97 ms /     9 tokens (   66.33 ms per token,    15.08 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     604.31 ms /    10 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     793.92 ms /    16 tokens (   49.62 ms per token,    20.15 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     802.78 ms /    17 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     849.16 ms /    15 tokens (   56.61 ms per token,    17.66 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     857.82 ms /    16 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     679.49 ms /    11 tokens (   61.77 ms per token,    16.19 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     686.64 ms /    12 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     977.62 ms /    18 tokens (   54.31 ms per token,    18.41 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     987.78 ms /    19 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     918.53 ms /    18 tokens (   51.03 ms per token,    19.60 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     925.09 ms /    19 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     858.28 ms /    16 tokens (   53.64 ms per token,    18.64 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     867.54 ms /    17 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     839.88 ms /    14 tokens (   59.99 ms per token,    16.67 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     848.36 ms /    15 tokens\n",
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     374.88 ms /     7 tokens (   53.55 ms per token,    18.67 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     380.05 ms /     8 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'input': 'Get the details of the employee with employee number 1002.',\n",
       "  'query': 'SELECT * FROM employees WHERE employeeNumber = 1002;'},\n",
       " {'input': 'Get the contact details of customers who have a credit limit greater than 100,000.',\n",
       "  'query': 'SELECT customerName, contactLastName, contactFirstName, phone FROM customers WHERE creditLimit > 100000;'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "\n",
    "# create example selector using vector search\n",
    "vectorstore = Chroma()\n",
    "vectorstore.delete_collection()\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,\n",
    "    embeddings,\n",
    "    vectorstore,\n",
    "    k=2,\n",
    "    input_keys=[\"input\"],\n",
    ")\n",
    "example_selector.select_examples({\"input\": \"how many employees we have?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     668.50 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     909.35 ms /     7 tokens (  129.91 ms per token,     7.70 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     915.05 ms /     8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Get the details of the employee with employee number 1002.\n",
      "SQLQuery:\n",
      "AI: SELECT * FROM employees WHERE employeeNumber = 1002;\n",
      "Human: Get the contact details of customers who have a credit limit greater than 100,000.\n",
      "SQLQuery:\n",
      "AI: SELECT customerName, contactLastName, contactFirstName, phone FROM customers WHERE creditLimit > 100000;\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder,FewShotChatMessagePromptTemplate,PromptTemplate\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\\nSQLQuery:\"),\n",
    "        (\"ai\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    input_variables=[\"input\",\"top_k\"],\n",
    ")\n",
    "print(few_shot_prompt.format(input=\"How many products are there?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a MySQL expert. Given an input question, create a syntactically correct MySQL query to run. Unless otherwise specificed.\\n\\nHere is the relevant table info: {table_info}\\n\\nBelow are a number of examples of questions and their corresponding SQL queries.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "print(final_prompt.format(input=\"How many products are there?\",table_info=\"some table info\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", table_details_prompt),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     533.01 ms\n",
      "llama_print_timings:      sample time =      11.38 ms /   389 runs   (    0.03 ms per token, 34191.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =      43.53 ms /    10 tokens (    4.35 ms per token,   229.75 tokens per second)\n",
      "llama_print_timings:        eval time =    6083.83 ms /   388 runs   (   15.68 ms per token,    63.78 tokens per second)\n",
      "llama_print_timings:       total time =    6421.09 ms /   398 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"To provide details of a customer and their orders, we might need to query the following tables:\\n\\n1. customers: This table contains customer information, such as contact details, address, credit limit, and sales representative.\\n2. orders: This table stores order information, including order dates, required dates, shipped dates, status, and customer numbers.\\n3. orderdetails: This table records order details, including product codes, quantities, prices, and line numbers.\\n\\nTo get the required information, you can use a SQL query like this:\\n\\n```sql\\nSELECT \\n    customers.customer_name, \\n    customers.contact_name, \\n    customers.address_line1, \\n    customers.city, \\n    customers.postal_code, \\n    customers.country, \\n    customers.credit_limit, \\n    customers.sales_representative, \\n    orders.order_date, \\n    orders.required_date, \\n    orders.shipped_date, \\n    orders.status, \\n    orderdetails.product_code, \\n    orderdetails.quantity, \\n    orderdetails.price, \\n    orderdetails.line_number\\nFROM \\n    customers\\nINNER JOIN \\n    orders \\nON \\n    customers.customer_number = orders.customer_number\\nINNER JOIN \\n    orderdetails \\nON \\n    orders.order_number = orderdetails.order_number\\n```\\n\\nThis query will return the customer's name, contact name, address, city, postal code, country, credit limit, sales representative, order date, required date, shipped date, status, product code, quantity, price, and line number for each order placed by the customer.\", response_metadata={'finish_reason': 'stop'}, id='run-bdf2c940-12d7-4078-8c54-99afd5b9bc42-0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing = prompt | llm\n",
    "testing.invoke({\"input\": \"give me details of customer and what they have orders\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to read a csv file containing table name and table description as polars dataframe, then output each as string of text with fromat table name: xxx \\n table description: xxx\n",
    "import polars as pl\n",
    "def get_table_info(file_path: str):\n",
    "    df = pl.read_csv(file_path)\n",
    "    table_info = \"\"\n",
    "    for i in range(len(df)):\n",
    "        table_info += f\"Table Name: {df['Table Name'][i]}\\nTable Description: {df['Description'][i]}\\n\\n\"\n",
    "    return table_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Name: customers\n",
      "Table Description: Stores customer information, including contact details, address, credit limit, and sales representative.\n",
      "\n",
      "Table Name: employees\n",
      "Table Description: Contains employee data, such as names, extensions, email addresses, job titles, and office codes.\n",
      "\n",
      "Table Name: offices\n",
      "Table Description: Holds office information, including city, phone number, address, state, country, and postal code.\n",
      "\n",
      "Table Name: orderdetails\n",
      "Table Description: Records order details, including product codes, quantities, prices, and line numbers.\n",
      "\n",
      "Table Name: orders\n",
      "Table Description: Stores order information, including order dates, required dates, shipped dates, status, and customer numbers.\n",
      "\n",
      "Table Name: payments\n",
      "Table Description: Tracks payment details, including check numbers, payment dates, and amounts.\n",
      "\n",
      "Table Name: productlines\n",
      "Table Description: Describes product lines, with text and HTML descriptions, and optional images.\n",
      "\n",
      "Table Name: products\n",
      "Table Description: Contains product data, including product codes, names, scales, vendors, descriptions, stock quantities, buy prices, and MSRP.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_info = get_table_info(\"tables_description.csv\")\n",
    "print(table_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "class Table(BaseModel):\n",
    "    \"\"\"Table in SQL database.\"\"\"\n",
    "\n",
    "    name: str = Field(description=\"Name of table in SQL database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_details_prompt = f\"\"\"Return the only the names of ALL the SQL tables that MIGHT be relevant to the user question using the information in Table Descritption. The tables are: \\\n",
    "\n",
    "{table_info}\n",
    "Remember to include ALL POTENTIALLY RELEVANT tables, even if you're not sure that they're needed. Only output the PORTENTIALLY RELEVANT Table Names and nothing else. Here is the user questions:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return the only the names of ALL the tables that MIGHT be relevant to the user question in json format. The tables are:\n",
      "\n",
      "Table Name: customers\n",
      "Table Description: Stores customer information, including contact details, address, credit limit, and sales representative.\n",
      "\n",
      "Table Name: employees\n",
      "Table Description: Contains employee data, such as names, extensions, email addresses, job titles, and office codes.\n",
      "\n",
      "Table Name: offices\n",
      "Table Description: Holds office information, including city, phone number, address, state, country, and postal code.\n",
      "\n",
      "Table Name: orderdetails\n",
      "Table Description: Records order details, including product codes, quantities, prices, and line numbers.\n",
      "\n",
      "Table Name: orders\n",
      "Table Description: Stores order information, including order dates, required dates, shipped dates, status, and customer numbers.\n",
      "\n",
      "Table Name: payments\n",
      "Table Description: Tracks payment details, including check numbers, payment dates, and amounts.\n",
      "\n",
      "Table Name: productlines\n",
      "Table Description: Describes product lines, with text and HTML descriptions, and optional images.\n",
      "\n",
      "Table Name: products\n",
      "Table Description: Contains product data, including product codes, names, scales, vendors, descriptions, stock quantities, buy prices, and MSRP.\n",
      "\n",
      "\n",
      "Remember to include ALL POTENTIALLY RELEVANT tables, even if you're not sure that they're needed. Here is the user questions:\n"
     ]
    }
   ],
   "source": [
    "print(table_details_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: LangChain has introduced a method called `with_structured_output` thatis available on ChatModels capable of tool calling.You can read more about the method here: <https://python.langchain.com/docs/modules/model_io/chat/structured_output/>. Please follow our extraction use case documentation for more guidelineson how to do information extraction with LLMs.<https://python.langchain.com/docs/use_cases/extraction/>. with_structured_output does not currently support a list of pydantic schemas. If this is a blocker or if you notice other issues, please provide feedback here:<https://github.com/langchain-ai/langchain/discussions/18154>\n",
      "  warn_deprecated(\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     533.01 ms\n",
      "llama_print_timings:      sample time =       8.64 ms /   292 runs   (    0.03 ms per token, 33788.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     219.15 ms /   315 tokens (    0.70 ms per token,  1437.36 tokens per second)\n",
      "llama_print_timings:        eval time =    4486.26 ms /   291 runs   (   15.42 ms per token,    64.86 tokens per second)\n",
      "llama_print_timings:       total time =    4907.80 ms /   606 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.openai_tools import create_extraction_chain_pydantic\n",
    "table_details_prompt = f\"\"\"Return the names of ALL the SQL tables that MIGHT be relevant to the user question. \\\n",
    "The tables are:\n",
    "\n",
    "{table_info}\n",
    "\n",
    "Remember to include ALL POTENTIALLY RELEVANT tables, even if you're not sure that they're needed.\"\"\"\n",
    "\n",
    "table_chain = create_extraction_chain_pydantic(Table, llm, system_message=table_details_prompt)\n",
    "tables = table_chain.invoke({\"input\": \"give me details of customer and their orders\"})\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_chain.get_prompts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
    "\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "tools = toolkit.get_tools()\n",
    "\n",
    "list_tables_tool = next(tool for tool in tools if tool.name == \"sql_db_list_tables\")\n",
    "get_schema_tool = next(tool for tool in tools if tool.name == \"sql_db_schema\")\n",
    "\n",
    "tables=list_tables_tool.invoke(\"\")\n",
    "print(tables)\n",
    "# split the string by \", \"\n",
    "tables = tables.split(\", \")\n",
    "\n",
    "for table in tables:\n",
    "    print(get_schema_tool.invoke(table))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool in tools:\n",
    "    print(tool.name, tool.description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
