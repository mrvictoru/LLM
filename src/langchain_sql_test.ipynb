{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell if langsmith is used to trace the chain\n",
    "import os\n",
    "from helper import get_api_key\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = get_api_key(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 25 key-value pairs and 273 tensors from ./models/xLAM-7b-fc-r.Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = xlam-FC-7b\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 30\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 102400\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = deepseek-llm\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,102400]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,99757]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"i n\", \"h e...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 100000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 100015\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 100001\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% set system_message = 'You are an A...\n",
      "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   61 tensors\n",
      "llama_model_loader: - type q4_K:  204 tensors\n",
      "llama_model_loader: - type q5_K:    7 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 2400\n",
      "llm_load_vocab: token to piece cache size = 0.6658 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 102400\n",
      "llm_load_print_meta: n_merges         = 99757\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 30\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 6.91 B\n",
      "llm_load_print_meta: model size       = 3.75 GiB (4.66 BPW) \n",
      "llm_load_print_meta: general.name     = xlam-FC-7b\n",
      "llm_load_print_meta: BOS token        = 100000 '<｜begin▁of▁sentence｜>'\n",
      "llm_load_print_meta: EOS token        = 100015 '<|EOT|>'\n",
      "llm_load_print_meta: PAD token        = 100001 '<｜end▁of▁sentence｜>'\n",
      "llm_load_print_meta: LF token         = 126 'Ä'\n",
      "llm_load_print_meta: max token length = 256\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.26 MiB\n",
      "llm_load_tensors: offloading 30 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 31/31 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   225.00 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  3610.09 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 10016\n",
      "llama_new_context_with_model: n_batch    = 600\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  4695.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4695.00 MiB, K (f16): 2347.50 MiB, V (f16): 2347.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.39 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   677.57 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    27.57 MiB\n",
      "llama_new_context_with_model: graph nodes  = 966\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set system_message = 'You are an AI assistant for function calling.For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\\\\n' %}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '### Instruction:\\\\n' + content + '\\\\n### Response:' }}{% elif message['role'] == 'assistant' %}{{ '\\\\n' + content + '\\\\n<|EOT|>\\\\n' }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '100001', 'tokenizer.ggml.eos_token_id': '100015', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.pre': 'deepseek-llm', 'llama.context_length': '4096', 'general.name': 'xlam-FC-7b', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.bos_token_id': '100000', 'llama.attention.head_count': '32', 'llama.block_count': '30', 'llama.attention.head_count_kv': '32', 'general.file_type': '14', 'llama.vocab_size': '102400', 'llama.rope.dimension_count': '128'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set system_message = 'You are an AI assistant for function calling.For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\\n' %}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '### Instruction:\\n' + content + '\\n### Response:' }}{% elif message['role'] == 'assistant' %}{{ '\\n' + content + '\\n<|EOT|>\\n' }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|EOT|>\n",
      "Using chat bos_token: <｜begin▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "# using local installed llama_cpp_python\n",
    "import multiprocessing\n",
    "\n",
    "from langchain_community.chat_models import ChatLlamaCpp\n",
    "\n",
    "local_model = \"./models/xLAM-7b-fc-r.Q4_K_S.gguf\"\n",
    "\n",
    "llm = ChatLlamaCpp(\n",
    "    temperature=0.2,\n",
    "    model_path=local_model,\n",
    "    n_ctx=10000,\n",
    "    n_gpu_layers=80,\n",
    "    n_batch=600,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    max_tokens=4096,\n",
    "    n_threads=multiprocessing.cpu_count() - 1,\n",
    "    repeat_penalty=1.5,\n",
    "    top_p=0.5,\n",
    "    verbose=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_base=\"http://llama_server:8080/\",\n",
    "    temperature=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mysql\n",
      "['customers', 'employees', 'offices', 'orderdetails', 'orders', 'payments', 'productlines', 'products']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[('S10_1678', '1969 Harley Davidson Ultimate Chopper', 'Motorcycles', '1:10', 'Min Lin Diecast', 'This replica features working kickstand, front suspension, gear-shift lever, footbrake lever, drive chain, wheels and steering. All parts are particularly delicate due to their precise scale and require special care and attention.', 7933, Decimal('48.81'), Decimal('95.70')), ('S10_1949', '1952 Alpine Renault 1300', 'Classic Cars', '1:10', 'Classic Metal Creations', 'Turnable front wheels; steering function; detailed interior; detailed engine; opening hood; opening trunk; opening doors; and detailed chassis.', 7305, Decimal('98.58'), Decimal('214.30')), ('S10_2016', '1996 Moto Guzzi 1100i', 'Motorcycles', '1:10', 'Highway 66 Mini Classics', 'Official Moto Guzzi logos and insignias, saddle bags located on side of motorcycle, detailed engine, working steering, working suspension, two leather seats, luggage rack, dual exhaust pipes, small saddle bag located on handle bars, two-tone paint with chrome accents, superior die-cast detail ,...', 6625, Decimal('68.99'), Decimal('118.94')), ('S10_4698', '2003 Harley-Davidson Eagle Drag Bike', 'Motorcycles', '1:10', 'Red Start Diecast', 'Model features, official Harley Davidson logos and insignias, detachable rear wheelie bar, heavy diecast metal with resin parts, authentic multi-color tampo-printed graphics, separate engine drive belts, free-turning front fork, rotating tires and rear racing slick, certificate of authenticity,...', 5582, Decimal('91.02'), Decimal('193.66')), ('S10_4757', '1972 Alfa Romeo GTA', 'Classic Cars', '1:10', 'Motor City Art Classics', 'Features include: Turnable front wheels; steering function; detailed interior; detailed engine; opening hood; opening trunk; opening doors; and detailed chassis.', 3252, Decimal('85.68'), Decimal('136.00'))]\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "\n",
    "db = SQLDatabase.from_uri(\"mysql://user:password@mysql_db:3306/classicmodels\")\n",
    "print(db.dialect)\n",
    "print(db.get_usable_table_names())\n",
    "db.run(\"SELECT * FROM products LIMIT 5;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[(Decimal('48.81'),)]\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.run(\"SELECT buyPrice from products where productName = '1969 Harley Davidson Ultimate Chopper';\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[('Jeff', 'Firrelli', 'VP Marketing')]\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.run(\"SELECT firstName, lastName, jobTitle FROM employees WHERE jobTitle LIKE '%Marketing%';\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_sql_query_chain\n",
    "\n",
    "generate_query = create_sql_query_chain(llm,db)\n",
    "query = generate_query.invoke({\"question\": \"What is the price of the '1969 Harley Davidson Ultimate Chopper'?\"})\n",
    "print(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "execute_query = QuerySQLDataBaseTool(db=db)\n",
    "execute_query.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     340.28 ms\n",
      "llama_print_timings:      sample time =      18.07 ms /    24 runs   (    0.75 ms per token,  1328.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =      45.79 ms /    26 tokens (    1.76 ms per token,   567.86 tokens per second)\n",
      "llama_print_timings:        eval time =     357.54 ms /    23 runs   (   15.55 ms per token,    64.33 tokens per second)\n",
      "llama_print_timings:       total time =     435.84 ms /    49 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[(Decimal('48.81'),)]\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_chain = generate_query | execute_query\n",
    "test_chain.invoke({\"question\": \"What is the price of the '1969 Harley Davidson Ultimate Chopper'?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a MySQL expert. Given an input question, first create a syntactically correct MySQL query to run, then look at the results of the query and return the answer to the input question.\n",
      "Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per MySQL. You can order the results to return the most informative data in the database.\n",
      "Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in backticks (`) to denote them as delimited identifiers.\n",
      "Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
      "Pay attention to use CURDATE() function to get the current date, if the question involves \"today\".\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: Question here\n",
      "SQLQuery: SQL Query to run\n",
      "SQLResult: Result of the SQLQuery\n",
      "Answer: Final answer here\n",
      "\n",
      "Only use the following tables:\n",
      "\u001b[33;1m\u001b[1;3m{table_info}\u001b[0m\n",
      "\n",
      "Question: \u001b[33;1m\u001b[1;3m{input}\u001b[0m\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(test_chain.get_prompts()[0].pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     320.63 ms\n",
      "llama_print_timings:      sample time =      18.21 ms /    24 runs   (    0.76 ms per token,  1318.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1417.89 ms /  2855 tokens (    0.50 ms per token,  2013.55 tokens per second)\n",
      "llama_print_timings:        eval time =     360.97 ms /    23 runs   (   15.69 ms per token,    63.72 tokens per second)\n",
      "llama_print_timings:       total time =    1812.61 ms /  2878 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     320.63 ms\n",
      "llama_print_timings:      sample time =      21.63 ms /    26 runs   (    0.83 ms per token,  1201.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =      42.34 ms /    88 tokens (    0.48 ms per token,  2078.36 tokens per second)\n",
      "llama_print_timings:        eval time =     265.11 ms /    25 runs   (   10.60 ms per token,    94.30 tokens per second)\n",
      "llama_print_timings:       total time =     340.65 ms /   113 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The buy Price for a new '1969 Harlely Davison ultimate choppers' is $50.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Result: {result}\n",
    "Answer: \"\"\"\n",
    ")\n",
    "\n",
    "# create first chain, where the template prompt get passed to the llm, the output is extract as str with output parser\n",
    "rephrase_answer = answer_prompt | llm | StrOutputParser()\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(query=generate_query).assign(\n",
    "        result=itemgetter(\"query\") | execute_query\n",
    "    )\n",
    "    | rephrase_answer\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"What is the price of the '1969 Harley Davidson Ultimate Chopper'?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a MySQL expert. Given an input question, first create a syntactically correct MySQL query to run, then look at the results of the query and return the answer to the input question.\n",
      "Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per MySQL. You can order the results to return the most informative data in the database.\n",
      "Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in backticks (`) to denote them as delimited identifiers.\n",
      "Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
      "Pay attention to use CURDATE() function to get the current date, if the question involves \"today\".\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: Question here\n",
      "SQLQuery: SQL Query to run\n",
      "SQLResult: Result of the SQLQuery\n",
      "Answer: Final answer here\n",
      "\n",
      "Only use the following tables:\n",
      "\u001b[33;1m\u001b[1;3m{table_info}\u001b[0m\n",
      "\n",
      "Question: \u001b[33;1m\u001b[1;3m{input}\u001b[0m\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(chain.get_prompts()[0].pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
      "\n",
      "Question: \u001b[33;1m\u001b[1;3m{question}\u001b[0m\n",
      "SQL Query: \u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "SQL Result: \u001b[33;1m\u001b[1;3m{result}\u001b[0m\n",
      "Answer: \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(chain.get_prompts()[1].pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of example prompts from sql_examples.json\n",
    "# read json file\n",
    "import json\n",
    "with open(\"sql_examples.json\", \"r\") as read_file:\n",
    "    examples = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customers, employees, offices, orderdetails, orders, payments, productlines, products\n",
      "\n",
      "CREATE TABLE customers (\n",
      "\t`customerNumber` INTEGER NOT NULL, \n",
      "\t`customerName` VARCHAR(50) NOT NULL, \n",
      "\t`contactLastName` VARCHAR(50) NOT NULL, \n",
      "\t`contactFirstName` VARCHAR(50) NOT NULL, \n",
      "\tphone VARCHAR(50) NOT NULL, \n",
      "\t`addressLine1` VARCHAR(50) NOT NULL, \n",
      "\t`addressLine2` VARCHAR(50), \n",
      "\tcity VARCHAR(50) NOT NULL, \n",
      "\tstate VARCHAR(50), \n",
      "\t`postalCode` VARCHAR(15), \n",
      "\tcountry VARCHAR(50) NOT NULL, \n",
      "\t`salesRepEmployeeNumber` INTEGER, \n",
      "\t`creditLimit` DECIMAL(10, 2), \n",
      "\tPRIMARY KEY (`customerNumber`), \n",
      "\tCONSTRAINT customers_ibfk_1 FOREIGN KEY(`salesRepEmployeeNumber`) REFERENCES employees (`employeeNumber`)\n",
      ")ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE utf8mb4_0900_ai_ci\n",
      "\n",
      "/*\n",
      "3 rows from customers table:\n",
      "customerNumber\tcustomerName\tcontactLastName\tcontactFirstName\tphone\taddressLine1\taddressLine2\tcity\tstate\tpostalCode\tcountry\tsalesRepEmployeeNumber\tcreditLimit\n",
      "103\tAtelier graphique\tSchmitt\tCarine \t40.32.2555\t54, rue Royale\tNone\tNantes\tNone\t44000\tFrance\t1370\t21000.00\n",
      "112\tSignal Gift Stores\tKing\tJean\t7025551838\t8489 Strong St.\tNone\tLas Vegas\tNV\t83030\tUSA\t1166\t71800.00\n",
      "114\tAustralian Collectors, Co.\tFerguson\tPeter\t03 9520 4555\t636 St Kilda Road\tLevel 3\tMelbourne\tVictoria\t3004\tAustralia\t1611\t117300.00\n",
      "*/\n",
      "\n",
      "CREATE TABLE employees (\n",
      "\t`employeeNumber` INTEGER NOT NULL, \n",
      "\t`lastName` VARCHAR(50) NOT NULL, \n",
      "\t`firstName` VARCHAR(50) NOT NULL, \n",
      "\textension VARCHAR(10) NOT NULL, \n",
      "\temail VARCHAR(100) NOT NULL, \n",
      "\t`officeCode` VARCHAR(10) NOT NULL, \n",
      "\t`reportsTo` INTEGER, \n",
      "\t`jobTitle` VARCHAR(50) NOT NULL, \n",
      "\tPRIMARY KEY (`employeeNumber`), \n",
      "\tCONSTRAINT employees_ibfk_1 FOREIGN KEY(`reportsTo`) REFERENCES employees (`employeeNumber`), \n",
      "\tCONSTRAINT employees_ibfk_2 FOREIGN KEY(`officeCode`) REFERENCES offices (`officeCode`)\n",
      ")ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE utf8mb4_0900_ai_ci\n",
      "\n",
      "/*\n",
      "3 rows from employees table:\n",
      "employeeNumber\tlastName\tfirstName\textension\temail\tofficeCode\treportsTo\tjobTitle\n",
      "1002\tMurphy\tDiane\tx5800\tdmurphy@classicmodelcars.com\t1\tNone\tPresident\n",
      "1056\tPatterson\tMary\tx4611\tmpatterso@classicmodelcars.com\t1\t1002\tVP Sales\n",
      "1076\tFirrelli\tJeff\tx9273\tjfirrelli@classicmodelcars.com\t1\t1002\tVP Marketing\n",
      "*/\n",
      "\n",
      "CREATE TABLE offices (\n",
      "\t`officeCode` VARCHAR(10) NOT NULL, \n",
      "\tcity VARCHAR(50) NOT NULL, \n",
      "\tphone VARCHAR(50) NOT NULL, \n",
      "\t`addressLine1` VARCHAR(50) NOT NULL, \n",
      "\t`addressLine2` VARCHAR(50), \n",
      "\tstate VARCHAR(50), \n",
      "\tcountry VARCHAR(50) NOT NULL, \n",
      "\t`postalCode` VARCHAR(15) NOT NULL, \n",
      "\tterritory VARCHAR(10) NOT NULL, \n",
      "\tPRIMARY KEY (`officeCode`)\n",
      ")ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE utf8mb4_0900_ai_ci\n",
      "\n",
      "/*\n",
      "3 rows from offices table:\n",
      "officeCode\tcity\tphone\taddressLine1\taddressLine2\tstate\tcountry\tpostalCode\tterritory\n",
      "1\tSan Francisco\t+1 650 219 4782\t100 Market Street\tSuite 300\tCA\tUSA\t94080\tNA\n",
      "2\tBoston\t+1 215 837 0825\t1550 Court Place\tSuite 102\tMA\tUSA\t02107\tNA\n",
      "3\tNYC\t+1 212 555 3000\t523 East 53rd Street\tapt. 5A\tNY\tUSA\t10022\tNA\n",
      "*/\n",
      "\n",
      "CREATE TABLE orderdetails (\n",
      "\t`orderNumber` INTEGER NOT NULL, \n",
      "\t`productCode` VARCHAR(15) NOT NULL, \n",
      "\t`quantityOrdered` INTEGER NOT NULL, \n",
      "\t`priceEach` DECIMAL(10, 2) NOT NULL, \n",
      "\t`orderLineNumber` SMALLINT NOT NULL, \n",
      "\tPRIMARY KEY (`orderNumber`, `productCode`), \n",
      "\tCONSTRAINT orderdetails_ibfk_1 FOREIGN KEY(`orderNumber`) REFERENCES orders (`orderNumber`), \n",
      "\tCONSTRAINT orderdetails_ibfk_2 FOREIGN KEY(`productCode`) REFERENCES products (`productCode`)\n",
      ")ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE utf8mb4_0900_ai_ci\n",
      "\n",
      "/*\n",
      "3 rows from orderdetails table:\n",
      "orderNumber\tproductCode\tquantityOrdered\tpriceEach\torderLineNumber\n",
      "10100\tS18_1749\t30\t136.00\t3\n",
      "10100\tS18_2248\t50\t55.09\t2\n",
      "10100\tS18_4409\t22\t75.46\t4\n",
      "*/\n",
      "\n",
      "CREATE TABLE orders (\n",
      "\t`orderNumber` INTEGER NOT NULL, \n",
      "\t`orderDate` DATE NOT NULL, \n",
      "\t`requiredDate` DATE NOT NULL, \n",
      "\t`shippedDate` DATE, \n",
      "\tstatus VARCHAR(15) NOT NULL, \n",
      "\tcomments TEXT, \n",
      "\t`customerNumber` INTEGER NOT NULL, \n",
      "\tPRIMARY KEY (`orderNumber`), \n",
      "\tCONSTRAINT orders_ibfk_1 FOREIGN KEY(`customerNumber`) REFERENCES customers (`customerNumber`)\n",
      ")ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE utf8mb4_0900_ai_ci\n",
      "\n",
      "/*\n",
      "3 rows from orders table:\n",
      "orderNumber\torderDate\trequiredDate\tshippedDate\tstatus\tcomments\tcustomerNumber\n",
      "10100\t2003-01-06\t2003-01-13\t2003-01-10\tShipped\tNone\t363\n",
      "10101\t2003-01-09\t2003-01-18\t2003-01-11\tShipped\tCheck on availability.\t128\n",
      "10102\t2003-01-10\t2003-01-18\t2003-01-14\tShipped\tNone\t181\n",
      "*/\n",
      "\n",
      "CREATE TABLE payments (\n",
      "\t`customerNumber` INTEGER NOT NULL, \n",
      "\t`checkNumber` VARCHAR(50) NOT NULL, \n",
      "\t`paymentDate` DATE NOT NULL, \n",
      "\tamount DECIMAL(10, 2) NOT NULL, \n",
      "\tPRIMARY KEY (`customerNumber`, `checkNumber`), \n",
      "\tCONSTRAINT payments_ibfk_1 FOREIGN KEY(`customerNumber`) REFERENCES customers (`customerNumber`)\n",
      ")ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE utf8mb4_0900_ai_ci\n",
      "\n",
      "/*\n",
      "3 rows from payments table:\n",
      "customerNumber\tcheckNumber\tpaymentDate\tamount\n",
      "103\tHQ336336\t2004-10-19\t6066.78\n",
      "103\tJM555205\t2003-06-05\t14571.44\n",
      "103\tOM314933\t2004-12-18\t1676.14\n",
      "*/\n",
      "\n",
      "CREATE TABLE productlines (\n",
      "\t`productLine` VARCHAR(50) NOT NULL, \n",
      "\t`textDescription` VARCHAR(4000), \n",
      "\t`htmlDescription` MEDIUMTEXT, \n",
      "\timage MEDIUMBLOB, \n",
      "\tPRIMARY KEY (`productLine`)\n",
      ")ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE utf8mb4_0900_ai_ci\n",
      "\n",
      "/*\n",
      "3 rows from productlines table:\n",
      "productLine\ttextDescription\thtmlDescription\timage\n",
      "Classic Cars\tAttention car enthusiasts: Make your wildest car ownership dreams come true. Whether you are looking\tNone\tNone\n",
      "Motorcycles\tOur motorcycles are state of the art replicas of classic as well as contemporary motorcycle legends \tNone\tNone\n",
      "Planes\tUnique, diecast airplane and helicopter replicas suitable for collections, as well as home, office o\tNone\tNone\n",
      "*/\n",
      "\n",
      "CREATE TABLE products (\n",
      "\t`productCode` VARCHAR(15) NOT NULL, \n",
      "\t`productName` VARCHAR(70) NOT NULL, \n",
      "\t`productLine` VARCHAR(50) NOT NULL, \n",
      "\t`productScale` VARCHAR(10) NOT NULL, \n",
      "\t`productVendor` VARCHAR(50) NOT NULL, \n",
      "\t`productDescription` TEXT NOT NULL, \n",
      "\t`quantityInStock` SMALLINT NOT NULL, \n",
      "\t`buyPrice` DECIMAL(10, 2) NOT NULL, \n",
      "\t`MSRP` DECIMAL(10, 2) NOT NULL, \n",
      "\tPRIMARY KEY (`productCode`), \n",
      "\tCONSTRAINT products_ibfk_1 FOREIGN KEY(`productLine`) REFERENCES productlines (`productLine`)\n",
      ")ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE utf8mb4_0900_ai_ci\n",
      "\n",
      "/*\n",
      "3 rows from products table:\n",
      "productCode\tproductName\tproductLine\tproductScale\tproductVendor\tproductDescription\tquantityInStock\tbuyPrice\tMSRP\n",
      "S10_1678\t1969 Harley Davidson Ultimate Chopper\tMotorcycles\t1:10\tMin Lin Diecast\tThis replica features working kickstand, front suspension, gear-shift lever, footbrake lever, drive \t7933\t48.81\t95.70\n",
      "S10_1949\t1952 Alpine Renault 1300\tClassic Cars\t1:10\tClassic Metal Creations\tTurnable front wheels; steering function; detailed interior; detailed engine; opening hood; opening \t7305\t98.58\t214.30\n",
      "S10_2016\t1996 Moto Guzzi 1100i\tMotorcycles\t1:10\tHighway 66 Mini Classics\tOfficial Moto Guzzi logos and insignias, saddle bags located on side of motorcycle, detailed engine,\t6625\t68.99\t118.94\n",
      "*/\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
    "\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "tools = toolkit.get_tools()\n",
    "\n",
    "list_tables_tool = next(tool for tool in tools if tool.name == \"sql_db_list_tables\")\n",
    "get_schema_tool = next(tool for tool in tools if tool.name == \"sql_db_schema\")\n",
    "\n",
    "tables=list_tables_tool.invoke(\"\")\n",
    "print(tables)\n",
    "# split the string by \", \"\n",
    "tables = tables.split(\", \")\n",
    "\n",
    "for table in tables:\n",
    "    print(get_schema_tool.invoke(table))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sql_db_query Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.\n",
      "sql_db_schema Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3\n",
      "sql_db_list_tables Input is an empty string, output is a comma-separated list of tables in the database.\n",
      "sql_db_query_checker Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!\n"
     ]
    }
   ],
   "source": [
    "for tool in tools:\n",
    "    print(tool.name, tool.description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
